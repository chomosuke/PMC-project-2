\documentclass{article}

\usepackage{upgreek}

\usepackage{parskip}

\sloppy

\usepackage{amsmath} % actually amsopn
\makeatletter
\DeclareRobustCommand{\var}[1]{\begingroup\newmcodes@\mathit{#1}\endgroup}
\makeatother
\makeatletter
\DeclareRobustCommand{\varb}[1]{\begingroup\newmcodes@\mathbf{#1}\endgroup}
\makeatother

\usepackage{amsfonts}

\usepackage[a4paper, margin=1in]{geometry}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\chevrons{\langle}{\rangle}

\usepackage{graphicx}
\graphicspath{{./}}

\usepackage{color}

\definecolor{hyperref}{rgb}{0, 0, 0.4}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	urlcolor=hyperref,
	}

\usepackage[section]{placeins}

\newcommand{\n}{\ \-}

\usepackage{enumitem}

\usepackage[style=ieee]{biblatex}
\addbibresource{refs.bib}

\usepackage{algorithm}
\usepackage{algpseudocode}

\MakeRobust{\Call}

\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algtext*{Indent}
\algtext*{EndIndent}

\begin{document}

\section*{Introduction}
This report outlines and analyses an implementation of a message passing parallel algorithm for
k-means++ clustering and the n-body simulation.\\
The algorithm is implemented with Open MPI where a number of nodes collaborate and passes messages
to each other to carry out the task required.\\
The task is described as follows (n denotes the total number of points and m denotes the total
number of nodes):
\begin{enumerate}
	\item n points are sampled by all m nodes from a Gaussian mixture model (GMM).
	\item All nodes collaborate to cluster the points into m clusters using k-means++.
	\item Each cluster is moved into one single node so that a single node contains all points of a
		cluster.
	\item The nodes collaborate with each other to perform n-body simulation where each point
		represents a body with a mass of 1 and bodies are attracted to each other via gravity.
\end{enumerate}

The implementation for sampling all n points and clustering them with k-means++ are all rather
naive and straightforward. The interesting part is the n-body gravity simulation.

To reduce the simulation time, the Barnes-Hut simulation approximation algorithm is used so that
points over a certain distance away are simplified into a single center of mass when calculating the
force they exert on another point. To reduce the communication overhead, each node constructs a
Barnes-Hut tree and then prune the tree for each other nodes so that the tree only contains the
nodes that are actually needed for the calculation of points in each other node. This pruned tree
will from now on be referred to as the partial tree.

For node A to construct a partial tree to be sent to node B, we would like to make sure that any
node of the tree where \(s / d < \theta\) is kept, where \(s\) is the width of the region the node of
the tree represent, \(d\) is the distance between the center of mass of the node and the closest
point in cluster B to the center of mass to the node, and \(\theta\) is a predefined constant.
However, we do not want to calculate the closest points in cluster B to every node in the Barnes-Hut
tree for cluster A and that would be slow in both computation time and communication overhead.
Hence, we instead take the closest point in cluster B with respect to the center of mass of all
points in cluster A, and calculate a hyperplane that is perpendicular to the line through the center
of mass and the closest point, that goes through the closest point. Assuming that all points in
each cluster are closer to their center of mass than any points in other clusters, the distance from
any point within cluster A to this hyperplane should be shorter than the distance from point within
cluster A to any point within cluster B.

The pseudocode describing the algorithm is shown in Algorithm 1. Note that the pseudocode is written
in the perspective on one computing node.
\begin{algorithm}
\caption{}
\begin{algorithmic}[1]
	\State $m \gets$ number of compute nodes.
	\State $NodeIndex \gets$ the index for the compute node.
	\Comment This is equivalent to MPI\_Comm\_rank but 1 indexed.
	\State $N \gets$ number of points
	\State $D \gets$ number of dimensions
	\State $c \gets$ number of GMM components
	\For{$i \gets 1$ \textbf{to} $c$}
		\State $prob[i] \gets$ the probability of the $i^{th}$ GMM component
		\State $gmmc[i] \gets$ the $i^{th}$ GMM component
	\EndFor
	\For{$i \gets 1$ \textbf{to} $N / m$}
		\State $points[i] \gets
		\Call{Sample}{gmmc[\Call{SampleWeightedDiscreteDistribution}{prob}]}$
		\State \Comment $\Call{SampleWeightedDiscreteDistribution}{weights}$ generate a random
		number between $1$ and the length of $weights$ with the number $i$ having a probability of
		$weights[i]/\Call{Sum}{weights}$
	\EndFor
	\State $centroids[1] \gets \Call{ChooseRandomOne}{points}$  \Comment choose centroid for
	k-means++
	\For{$i \gets 2$ \textbf{to} $m$} \Comment choose $m$ centroids for $m$ clusters
		\For{$j \gets 1$ \textbf{to} $\Call{Len}{points}$}
			\State $distances[i] \gets$ distance of $points[i]$ to the closest centroid
		\EndFor{}
		\State $DistSum \gets$ sum of distances
		\If{$NodeIndex = 1$}
			\For{$i \gets 1$ \textbf{to} $m$}
				\State $AllSum[i] \gets DistSum$ from node $i$
			\EndFor
			\State $NextCentroidNode \gets \Call{SampleWeightedDiscreteDistribution}{AllSum}$
		\EndIf
		\State $NextCentroidNode \gets NextCentroidNode$ from node 1
		\If{$NodeIndex = NextCentroidNode$}
			\State $NextCentroid \gets points[\Call{SampleWeightedDiscreteDistribution}{distances}]$
		\EndIf
		\State $centroids[i] \gets NextCentroid$ from node $NextCentroidNode$
	\EndFor
	\State $ClusterIndices \gets \Call{KMeans}{centroids, points}$
	\State $points \gets$ points in $NodeIndex^{th}$ cluster as indicated by $ClusterIndices$ in all
	nodes \Comment Significant implementation detail of message passing between nodes omitted.
	
	\For{$i \gets 1$ \textbf{to} $\Call{Len}{points}$}
		\State $velocities[i] \gets 0$
	\EndFor

	\State $InitVariance \gets \Call{GetVariance}{points}$
	\State $ClusterIndices \gets \Call{KMeans}{centroids, points}$ \Comment one iteration only,
	implemented separately in real code but reusing pseudocode here.
	\State $points \gets$ points in $NodeIndex^{th}$ cluster as indicated by $ClusterIndices$ in all
	nodes
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{}
\begin{algorithmic}[1]
	\Function{KMeans}{centroids, points}
		\While{centroids changed since last iteration}
			\For{$i \gets 1$ \textbf{to} $\Call{Len}{points}$}
				\State $ClusterIndices[i] \gets$ index of centroid closest to $points$
			\EndFor
			\For{$i \gets 1$ \textbf{to} $\Call{Len}{points}$}
				\State $sums[ClusterIndices[i]] \gets sums[ClusterIndices[i]] + points[i]$
				\State $counts[ClusterIndices[i]] \gets counts[ClusterIndices[i]] + 1$
			\EndFor
			\If{NodeIndex == 1}
				\For{$i \gets 1$ \textbf{to} $\Call{Len}{centroids}$}
					\State $allSums \gets$ sum of $sums[i]$ from all nodes
					\State $allCounts \gets$ sum of $counts[i]$ from all nodes
					\State $centroids[i] \gets allSums / allCounts$ \Comment Here it is actually
					each dimension of $allSums$ divided by $allCounts$.
				\EndFor
			\EndIf
			\State $centroids \gets centroids$ from node $1$
		\EndWhile
		\State \Return $ClusterIndices$
	\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{}
\begin{algorithmic}[1]
	\Function{GetVariance}{points}
		\State $mean \gets$ average of all points
		\If{$NodeIndex = 1$}
			\State $sum \gets 0$
			\State $allSize \gets 0$
			\For{$i \gets 1$ \textbf{to} $m$}
				\State $means[i] \gets mean$ from node $i$
				\State $sizes[i] \gets \Call{Len}{points}$ from node $i$
				\State $sum \gets sum + means[i] * sizes[i]$
				\State $allSize \gets allSize + sizes[i]$
			\EndFor
			\State $allMean \gets sum / allSize$
		\EndIf
		\State $mean \gets allMean$ from node 1
		\State $variance \gets 0$
		\For{$i \gets 1$ \textbf{to} $\Call{Len}{points}$}
			\State $variance \gets variance + \Call{Distance}{points[i], mean}$
		\EndFor
		\State $allVariance \gets$ sum of all $variance$ from all nodes
		\State \Return $allVariance$
	\EndFunction
\end{algorithmic}
\end{algorithm}

While in theory, this algorithm should be faster than naively parallelizing Barnes-Hut algorithm by
transferring all points to a root cluster and calculating one single Barnes-Hut tree to be sent to
all clusters. In practice, it turns out the algorithm has a number of other overheads.

These overheads include calculating \(m\) partial trees (again here \(m\) is the total number of
computing nodes), encoding them into an array of bytes to be sent over to each node and decoding the
bytes into a partial tree, plus the extra computing time of creating \(m^2\) partial trees.
Furthermore, because each simulation step relies on the assumption of points of each all points in
each cluster are closer to their center of mass than any points in other clusters being true, the
clusters have to be recomputed and points that changed clusters sent over the corresponding nodes.
This adds additional overheads that, when combined with all the other overheads mentioned above,
might outweigh the benefit of not having to send all points to all nodes.

In the future, it would be interesting to compare the performance of the current implementation
against the naively parallelized Barnes-Hut algorithm described above.

Due to time constraint, the implementation is also not parallelized within a node. As a result, this
does not take advantage of the multicore nature of each node. In the future, it'll be interesting to
see how much extra performance can be extracted via parallelization within each node.

\section*{Methodology}


\section*{Experiments}

\section*{Analysis \& Discussion}

\end{document}
